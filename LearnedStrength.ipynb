{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b685e20-16c7-4ccd-8ed3-617ff018e516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/workspace/SteerKep/activation-steering')\n",
    "sys.path.append(\"/workspace/SteerKep/SteerPoser/src\")\n",
    "sys.path.append(\"/workspace/SteerKep/steer-data\")\n",
    "sys.path.append(\"/workspace/SteerKep/RLBench\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e017de25-4400-40af-aa14-7e3c04ff21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from activation_steering import SteeringDataset, MalleableModel, SteeringVector\n",
    "from arguments import get_config\n",
    "from user_steered_model import *\n",
    "steer_cfg = get_config(config_path='/workspace/SteerKep/steer-data/steerconfig.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52bb4df1-a28f-463b-a155-16382d426d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NousResearch/DeepHermes-3-Mistral-24B-Preview into HF cache dir /workspace/.cache/huggingface on device cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd5fabb8a804254ba584d763600da84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #800080; text-decoration-color: #800080\"> The target model type is </span><span style=\"color: #008080; text-decoration-color: #008080\">mistral</span><span style=\"color: #800080; text-decoration-color: #800080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[35m The target model type is \u001b[0m\u001b[36mmistral\u001b[0m\u001b[35m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = UserSteeredModel(steer_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a9116b-b6a4-4273-8b50-c60eee6987e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mall = model.malleable_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c7593-9795-4cf2-88e9-827a724af1f7",
   "metadata": {},
   "source": [
    "# Accessing inner layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b3e8ba-0614-4e57-83aa-fd60c31e9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_steering import malleable_model as mm\n",
    "from activation_steering import steering_vector as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f2876ee-3ac9-48d4-8a5a-a4aa6d6e3cf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UserSteeredModel' object has no attribute 'steering_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sv = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msteering_vector\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'UserSteeredModel' object has no attribute 'steering_vector'"
     ]
    }
   ],
   "source": [
    "sv = model.steering_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de901d6f-3be2-49f0-9867-b76d20b48a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cf61930-f6b6-4cd4-8ea3-5132a7c4b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_chat_template = False\n",
    "prompt = [\"i went to the store and got\"]\n",
    "self = mall\n",
    "settings = model.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27857d81-1cb3-4af6-887e-6aa4dc12811a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mall.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "461e009d-1e68-42ed-b142-2737d31d2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_chat_template:\n",
    "    formatted_prompt = self.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{prompt}\"}],\n",
    "        tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "else:\n",
    "    formatted_prompt = prompt\n",
    "\n",
    "input_ids = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "if settings is None:\n",
    "    settings = {\n",
    "        \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "        \"do_sample\": False,\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "    }\n",
    "\n",
    "with torch.no_grad():  # Ensure we're not tracking gradients during inference\n",
    "    output = self.model.generate(**input_ids, **settings)\n",
    "\n",
    "response = self.tokenizer.decode(output.squeeze()[input_ids['input_ids'].shape[1]:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "568833b8-5b97-406a-8794-d27c3e81061b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i went to the store and got']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d725b29d-90c8-435b-b6cd-61e3729599e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     1,      1,   1060,   1124,  10460,  35645,   3384,   1124,   1062,\n",
       "           3263,   1060,   1124,   1474,  35645,   3384,   1124,   3318,   3024,\n",
       "           1105,   6387,   1317,   1278,   8141,   1321,   4884,   2963, 131072,\n",
       "           1060,   1124,  10460,  35645,   3384,   1124,   1062,   1503,  19464,\n",
       "           1060,   1124,   1474,  35645,   3384,   1124,   3318]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a390f0d-fe59-4d81-9be6-03cb37da0e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It seems like you're sharing a sentence fragment. Could you please provide more context or complete the sentence so I can better assist you?<|eot_id|>\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e14dbb10-4703-4bd8-9052-6f4ab900b2e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mi went to the store and got\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/transformers/generation/utils.py:2243\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[32m   2240\u001b[39m inputs_tensor, model_input_name, model_kwargs = \u001b[38;5;28mself\u001b[39m._prepare_model_inputs(\n\u001b[32m   2241\u001b[39m     inputs, generation_config.bos_token_id, model_kwargs\n\u001b[32m   2242\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2243\u001b[39m batch_size = \u001b[43minputs_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   2245\u001b[39m device = inputs_tensor.device\n\u001b[32m   2246\u001b[39m \u001b[38;5;28mself\u001b[39m._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "mall.model.generate([\"i went to the store and got\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d66598af-1b42-4be4-a7d0-59966fd05a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = mm.get_model_layer_list(mall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "178d7082-a7bc-455b-a777-8e67ab7311c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralDecoderLayer(\n",
       "  (self_attn): MistralAttention(\n",
       "    (q_proj): Linear(in_features=5120, out_features=4096, bias=False)\n",
       "    (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
       "    (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
       "    (o_proj): Linear(in_features=4096, out_features=5120, bias=False)\n",
       "  )\n",
       "  (mlp): MistralMLP(\n",
       "    (gate_proj): Linear(in_features=5120, out_features=32768, bias=False)\n",
       "    (up_proj): Linear(in_features=5120, out_features=32768, bias=False)\n",
       "    (down_proj): Linear(in_features=32768, out_features=5120, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): MistralRMSNorm((5120,), eps=1e-05)\n",
       "  (post_attention_layernorm): MistralRMSNorm((5120,), eps=1e-05)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[0].layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c356a-dbca-4715-8b91-0aef98aa3fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e158b-21db-40ce-8550-fc9149b6870e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc86af1b-9052-42ca-9445-ace35e081f1c",
   "metadata": {},
   "source": [
    "# Just playing with LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5912be9c-b81c-4e8d-8c9b-9816fc4446ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from steered_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1eb1e66f-0173-454f-a15a-af09d850ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NousResearch/DeepHermes-3-Mistral-24B-Preview into HF cache dir /workspace/.cache/huggingface on device cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading SteeringVector from <span style=\"color: #800080; text-decoration-color: #800080\">/workspace/SteerKep/SteerPoser/src/svec/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">junk-healthy-24b.svec</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading SteeringVector from \u001b[35m/workspace/SteerKep/SteerPoser/src/svec/\u001b[0m\u001b[95mjunk-healthy-24b.svec\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded directions for layers: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>, \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">38</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded directions for layers: \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m9\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[1;36m13\u001b[0m, \u001b[1;36m14\u001b[0m, \u001b[1;36m15\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m17\u001b[0m, \u001b[1;36m18\u001b[0m, \u001b[1;36m19\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m21\u001b[0m, \u001b[1;36m22\u001b[0m, \n",
       "\u001b[1;36m23\u001b[0m, \u001b[1;36m24\u001b[0m, \u001b[1;36m25\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m27\u001b[0m, \u001b[1;36m28\u001b[0m, \u001b[1;36m29\u001b[0m, \u001b[1;36m30\u001b[0m, \u001b[1;36m31\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m33\u001b[0m, \u001b[1;36m34\u001b[0m, \u001b[1;36m35\u001b[0m, \u001b[1;36m36\u001b[0m, \u001b[1;36m37\u001b[0m, \u001b[1;36m38\u001b[0m, \u001b[1;36m39\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Shape of first direction vector: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5120</span>,<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Shape of first direction vector: \u001b[1m(\u001b[0m\u001b[1;36m5120\u001b[0m,\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #800080; text-decoration-color: #800080\"> The target model type is </span><span style=\"color: #008080; text-decoration-color: #008080\">mistral</span><span style=\"color: #800080; text-decoration-color: #800080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[35m The target model type is \u001b[0m\u001b[36mmistral\u001b[0m\u001b[35m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Steering</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSteering\u001b[0m\u001b[1;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2 = SteeredModel(model.cfg, model.model, model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb160a0-b576-42d8-883d-7cf2236b2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.update_steer(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab70859b-a570-4e17-b7db-9926a80f0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsteer = model2.generate(\"Went to grocery store and bought\", steer=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98b14070-89d3-4ef9-b242-242983a9f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "steer = model2.generate(\"Went to grocery store and bought\", steer=True, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "675f0360-958c-4171-8877-73aa498d15ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It seems like you're looking for a list of items you bought at the grocery store. However, you haven't provided any specific items or details about your shopping. Could you please provide more information about what you purchased or what you need help with?\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a4157d8-5883-43c3-9978-3f9dfeca3995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Went to grocery store and bought some fresh fruits and vegetables. I also bought some fresh fish and some fresh meat. I also bought some fresh herbs and spices. I also bought some fresh nuts and seeds. I also bought some fresh yogurt and some fresh cheese. I also bought some fresh bread and some fresh pasta. I also bought some fresh fruit juice and some fresh vegetable juice. I also bought some fresh honey and some fresh maple syrup. I also bought some fresh almond milk and some fresh coconut milk. I also bought some fresh chia seeds and some fresh flax seeds. I also bought some fresh chia seeds and some fresh flax seeds. I also bought some fresh chia seeds and some fresh flax seeds. I also bought some fresh chia seeds and some fresh flax seeds. I also bought some fresh chia seeds and some fresh flax seeds. I also bought some fresh chia seeds and some fresh flax seeds. I also bought some fresh chia seeds and some fresh flax seeds. I also bought some fresh chia seeds and some fresh flax seeds. I also bought some fresh chia seeds and some fresh flax seeds. I also bought some fresh chia seeds and some fresh flax seeds. I also bought some fresh chia seeds and some fresh flax seeds. I also bought some'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "26efaa97-896a-486b-a628-c730aa16d187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/SteerKep/SteerPoser/src/svec/junk-healthy-24b'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_cfg.steering_vector_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5164358-3e73-4b6e-b16a-9ab41ada9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "SteeringVector.load(cfg.steering_vector_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a97e81d-60e4-466c-8a6e-1a3df76038b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteeredModel:\n",
    "    def __init__(self, cfg, model, tokenizer, verbose=True):\n",
    "        self.cfg = cfg\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if verbose:\n",
    "            print(f\"Loading {cfg.model_name} into HF cache dir {cfg.cache_dir} on device {device}\")\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        #main_device = model.hf_device_map.get(next(iter(self.model.hf_device_map)))\n",
    "        #main_device = torch.device(f\"cuda:{main_device}\") if isinstance(main_device, int) else torch.device(main_device)\n",
    "        self.steering_vector = SteeringVector.load(cfg.steering_vector_path)\n",
    "        self.malleable_model = MalleableModel(self.model, self.tokenizer)\n",
    "        self.malleable_model.steer(\n",
    "            self.steering_vector,\n",
    "            behavior_layer_ids=cfg.behavior_layer_ids,\n",
    "            behavior_vector_strength=cfg.behavior_vector_strength,\n",
    "        )\n",
    "        #self.malleable_model.device = main_device\n",
    "        self.settings = {\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"do_sample\": False,\n",
    "            \"max_new_tokens\": cfg.max_new_tokens,\n",
    "            \"eos_token_id\": self.tokenizer.eos_token_id\n",
    "        }\n",
    "        self.model.eval()\n",
    "\n",
    "    def update_steer(self, steer_vector, strength):\n",
    "        svpath = os.path.join(cfg.steering_vector_dir, steer_vector)\n",
    "        self.steering_vector = SteeringVector.load(svpath)\n",
    "        self.malleable_model.steer(\n",
    "            self.steering_vector,\n",
    "            behavior_layer_ids=cfg.behavior_layer_ids,\n",
    "            behavior_vector_strength=strength,\n",
    "        )\n",
    "        \n",
    "    def generate(self, prompts, steer=True, debug=False):\n",
    "        # Accept a single string or a list of strings\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "\n",
    "        if steer:\n",
    "            if(debug):\n",
    "                print(\"Steering!\\n\")\n",
    "            # Steered generation using MalleableModel\n",
    "            responses = self.malleable_model.respond_batch_sequential(prompts=prompts,settings=self.settings) \n",
    "        else:\n",
    "             # Unsteered generation using regular Hugging Face model\n",
    "             inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True).to(self.model.device)\n",
    "             outputs = self.model.generate(**inputs, **self.settings)\n",
    "             responses = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # # If user passed a single prompt, return a single string\n",
    "        # ret = []\n",
    "        # for response in responses:\n",
    "        #     ret.append(response.split('#')[0])\n",
    "\n",
    "        # print(\"from the steered model -\", ret)\n",
    "        if(debug):\n",
    "            print(\"raw output: \", responses[0], \"\\n\\n\")\n",
    "        responses = [parse_output(txt) for txt in responses]\n",
    "        responses = responses[0] if len(responses) == 1 else responses\n",
    "        return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d522ddce-a4c7-45a7-88d7-0e74d205fb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in steered model I went to the grocery store and bought a variety of fresh fruits and vegetables, lean proteins, whole grains, and healthy snacks. I also picked up some dairy products, nuts, seeds, and a few pantry staples like olive oil, spices, and herbs. I made sure to choose organic and locally sourced produce whenever possible to support local farmers and reduce my environmental impact. I also picked up some fresh herbs and spices to add flavor to my meals without adding extra salt or sugar. I'm excited to cook some healthy and delicious meals with my fresh ingredients!<|eot_id|>\n",
      "after parsing I went to the grocery store and bought a variety of fresh fruits and vegetables, lean proteins, whole grains, and healthy snacks. I also picked up some dairy products, nuts, seeds, and a few pantry staples like olive oil, spices, and herbs. I made sure to choose organic and locally sourced produce whenever possible to support local farmers and reduce my environmental impact. I also picked up some fresh herbs and spices to add flavor to my meals without adding extra salt or sugar. I'm excited to cook some healthy and delicious meals with my fresh ingredients!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I went to the grocery store and bought a variety of fresh fruits and vegetables, lean proteins, whole grains, and healthy snacks. I also picked up some dairy products, nuts, seeds, and a few pantry staples like olive oil, spices, and herbs. I made sure to choose organic and locally sourced produce whenever possible to support local farmers and reduce my environmental impact. I also picked up some fresh herbs and spices to add flavor to my meals without adding extra salt or sugar. I'm excited to cook some healthy and delicious meals with my fresh ingredients!\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"I went to the grocery store and bought a \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c25351-1b0f-4691-be7d-270b1a5b22e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11a90f-f072-4715-9a2e-0bbb7d97a4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ec70a38-3ee5-4745-ba94-c223bb09054f",
   "metadata": {},
   "source": [
    "# Moving to Composer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c1bd0b6-e1d7-47de-b003-4747558231f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "from time import sleep\n",
    "from openai._exceptions import RateLimitError, APIConnectionError\n",
    "from pygments import highlight\n",
    "from pygments.lexers import PythonLexer\n",
    "from pygments.formatters import TerminalFormatter\n",
    "from utils import load_prompt, DynamicObservation, IterableDynamicObservation\n",
    "import time\n",
    "from LLM_cache import DiskCache\n",
    "from arguments import get_config\n",
    "from LMP import *\n",
    "\n",
    "class LMP:\n",
    "    \"\"\"Language Model Program (LMP), adopted from Code as Policies.\"\"\"\n",
    "    def __init__(self, name, cfg, fixed_vars, variable_vars, debug=False, env='rlbench', model=None):\n",
    "        # fixed vars and variable vars define function interface available to LLM\n",
    "        # fixed vars are static, come from transforms3d library\n",
    "        # variable vars are VoxPoser defined functions, defined in interfaces.py.\n",
    "        #  Includes access to other low-level LMPs, i.e. planner can call composer. \n",
    "        self._name = name # composer, planner, affordance map generator, etc\n",
    "        self._cfg = cfg # cfg['lmp_config']['lmps'][name]\n",
    "        self._debug = debug\n",
    "        self._base_prompt = load_prompt(f\"{env}/{self._cfg['prompt_fname']}.txt\")\n",
    "        self._stop_tokens = list(self._cfg['stop'])\n",
    "        self._fixed_vars = fixed_vars\n",
    "        self._variable_vars = variable_vars\n",
    "        self.exec_hist = ''\n",
    "        self._context = None\n",
    "        self._cache = DiskCache(load_cache=self._cfg['load_cache'])\n",
    "        \n",
    "        \n",
    "        if self._cfg['use_local']: # whether steering with local model vs openai api call\n",
    "             if model is None:\n",
    "                 self.steer_cfg = get_config(config_path='./configs/steering.yaml')\n",
    "                 self.model = SteeredModel(self.steer_cfg)\n",
    "             else:\n",
    "                 print(f\"using passed in model for LMP {self._name}\")\n",
    "                 self.model = model\n",
    "\n",
    "    def clear_exec_hist(self):\n",
    "        self.exec_hist = ''\n",
    "\n",
    "    def build_prompt(self, query):\n",
    "        if len(self._variable_vars) > 0:\n",
    "            variable_vars_imports_str = f\"from utils import {', '.join(self._variable_vars.keys())}\"\n",
    "        else:\n",
    "            variable_vars_imports_str = ''\n",
    "        prompt = self._base_prompt.replace('{variable_vars_imports}', variable_vars_imports_str)\n",
    "\n",
    "        if self._cfg['maintain_session'] and self.exec_hist != '':\n",
    "            prompt += f'\\n{self.exec_hist}'\n",
    "        \n",
    "        prompt += '\\n'  # separate prompted examples with the query part\n",
    "\n",
    "        if self._cfg['include_context']:\n",
    "            assert self._context is not None, 'context is None'\n",
    "            prompt += f'\\n{self._context}'\n",
    "\n",
    "        user_query = f'{self._cfg[\"query_prefix\"]}{query}{self._cfg[\"query_suffix\"]}'\n",
    "        prompt += f'\\n{user_query}'\n",
    "\n",
    "        return prompt, user_query\n",
    "    \n",
    "    def _cached_api_call(self, **kwargs):\n",
    "        # check whether completion endpoint or chat endpoint is used\n",
    "        if kwargs['model'] != 'gpt-3.5-turbo-instruct' and any([chat_model in kwargs['model'] for chat_model in ['gpt-3.5', 'gpt-4']]):\n",
    "            # add special prompt for chat endpoint\n",
    "            user1 = kwargs.pop('prompt')\n",
    "            new_query = '# Query:' + user1.split('# Query:')[-1]\n",
    "            user1 = ''.join(user1.split('# Query:')[:-1]).strip()\n",
    "            user1 = f\"I would like you to help me write Python code to control a robot arm operating in a tabletop environment. Please complete the code every time when I give you new query. Pay attention to appeared patterns in the given context code. Be thorough and thoughtful in your code. Do not include any import statement. Do not repeat my question. Do not provide any text explanation (comment in code is okay). I will first give you the context of the code below:\\n\\n```\\n{user1}\\n```\\n\\nNote that x is back to front, y is left to right, and z is bottom to up.\"\n",
    "            assistant1 = f'Got it. I will complete what you give me next.'\n",
    "            user2 = new_query\n",
    "            # handle given context (this was written originally for completion endpoint)\n",
    "            if user1.split('\\n')[-4].startswith('objects = ['):\n",
    "                obj_context = user1.split('\\n')[-4]\n",
    "                # remove obj_context from user1\n",
    "                user1 = '\\n'.join(user1.split('\\n')[:-4]) + '\\n' + '\\n'.join(user1.split('\\n')[-3:])\n",
    "                # add obj_context to user2\n",
    "                user2 = obj_context.strip() + '\\n' + user2\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that pays attention to the user's instructions and writes good python code for operating a robot arm in a tabletop environment.\"},\n",
    "                {\"role\": \"user\", \"content\": user1},\n",
    "                {\"role\": \"assistant\", \"content\": assistant1},\n",
    "                {\"role\": \"user\", \"content\": user2},\n",
    "            ]\n",
    "            kwargs['messages'] = messages\n",
    "            # print kwargs to a file\n",
    "            \n",
    "            if kwargs in self._cache:\n",
    "                print('(using cache)', end=' ')\n",
    "                return self._cache[kwargs], kwargs\n",
    "            else:\n",
    "                response = openai.chat.completions.create(**kwargs)\n",
    "                ret = response.choices[0].message.content\n",
    "                # post processing\n",
    "                ret = ret.replace('```', '').replace('python', '').strip()\n",
    "                self._cache[kwargs] = ret\n",
    "                return ret, kwargs\n",
    "        else:\n",
    "            if kwargs in self._cache:\n",
    "                print('(using cache)', end=' ')\n",
    "                return self._cache[kwargs], kwargs\n",
    "            else:\n",
    "                response = openai.completions.create(**kwargs)['choices'][0]['text'].strip()\n",
    "                ret = response.choices[0].text.strip()\n",
    "                self._cache[kwargs] = ret\n",
    "                return ret, kwargs\n",
    "    \n",
    "    def _chat_template(self, messages):\n",
    "        prompt = ''\n",
    "        for message in messages:\n",
    "            if message['role'] == 'user':\n",
    "                prompt += f'{message[\"content\"]}\\n'\n",
    "            elif message['role'] == 'assistant':\n",
    "                prompt += f'{message[\"content\"]}\\n'\n",
    "        return prompt\n",
    "    \n",
    "    def _local_call(self, **kwargs):\n",
    "        user1 = kwargs.pop('prompt')\n",
    "        new_query = '# Query:' + user1.split('# Query:')[-1]\n",
    "        user1 = ''.join(user1.split('# Query:')[:-1]).strip()\n",
    "        user1 = f\"I would like you to help me write Python code to control a robot arm operating in a tabletop environment. Please complete the code every time when I give you new query. Pay attention to appeared patterns in the given context code. Be thorough and thoughtful in your code. Do not include any import statement. Do not repeat my question. Do not provide any text explanation (comment in code is okay). I will first give you the context of the code below:\\n\\n```\\n{user1}\\n```\\n\\nNote that x is back to front, y is left to right, and z is bottom to up. Your task is just to write clean and concise code that follows the template above based on the objects and query supplied below.\"\n",
    "        user2 = new_query\n",
    "        \n",
    "        if user1.split('\\n')[-4].startswith('objects = ['):\n",
    "            obj_context = user1.split('\\n')[-4]\n",
    "            user1 = '\\n'.join(user1.split('\\n')[:-4]) + '\\n' + '\\n'.join(user1.split('\\n')[-3:])\n",
    "            user2 = obj_context.strip() + '\\n' + user2\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that pays attention to the user's instructions and writes good python code for operating a robot arm in a tabletop environment.\"},\n",
    "            {\"role\": \"user\", \"content\": user1},\n",
    "            {\"role\": \"user\", \"content\": user2},\n",
    "        ]\n",
    "        kwargs['messages'] = messages\n",
    "\n",
    "        kwargs['local'] = True\n",
    "\n",
    "        if kwargs in self._cache:\n",
    "            print(\"using cached response yeehaw\")\n",
    "            return self._cache[kwargs], kwargs\n",
    "        \n",
    "        prompt = self._chat_template(messages)\n",
    "\n",
    "        # print(\"prompting model with\", prompt)\n",
    "        \n",
    "        ret = self.model.generate(prompt)\n",
    "\n",
    "        self._cache[kwargs] = ret\n",
    "        \n",
    "        return ret, kwargs\n",
    "\n",
    "    \n",
    "\n",
    "    def __call__(self, query, **kwargs):\n",
    "        prompt, user_query = self.build_prompt(query)\n",
    "\n",
    "        start_time = time.time()\n",
    "        while True:\n",
    "            try:\n",
    "                if self._cfg['use_local']:\n",
    "                    code_str, _ = self._local_call(\n",
    "                        prompt=prompt,\n",
    "                        stop=self._stop_tokens,\n",
    "                        temperature=self._cfg['temperature'],\n",
    "                        max_tokens=self._cfg['max_tokens']\n",
    "                    )\n",
    "                    print(f'*** Local call took {time.time() - start_time:.2f}s ***')\n",
    "                else:\n",
    "                    code_str, _ = self._cached_api_call(\n",
    "                        prompt=prompt,\n",
    "                        stop=self._stop_tokens,\n",
    "                        temperature=self._cfg['temperature'],\n",
    "                        model=self._cfg['model'],\n",
    "                        max_tokens=self._cfg['max_tokens']\n",
    "                    )\n",
    "                    print(f'*** OpenAI API call took {time.time() - start_time:.2f}s ***')\n",
    "                break\n",
    "            except (RateLimitError, APIConnectionError) as e:\n",
    "                print(f'OpenAI API got err {e}')\n",
    "                print('Retrying after 3s.')\n",
    "                sleep(3)\n",
    "\n",
    "        if self._cfg['include_context']:\n",
    "            assert self._context is not None, 'context is None'\n",
    "            to_exec = f'{self._context}\\n{code_str}'\n",
    "            to_log = f'{self._context}\\n{user_query}\\n{code_str}'\n",
    "        else:\n",
    "            to_exec = code_str\n",
    "            to_log = f'{user_query}\\n{to_exec}'\n",
    "\n",
    "        to_log_pretty = highlight(to_log, PythonLexer(), TerminalFormatter())\n",
    "\n",
    "        if self._cfg['include_context']:\n",
    "            print('#'*40 + f'\\n## \"{self._name}\" generated code\\n' + f'## context: \"{self._context}\"\\n' + '#'*40 + f'\\n{to_log_pretty}\\n')\n",
    "        else:\n",
    "            print('#'*40 + f'\\n## \"{self._name}\" generated code\\n' + '#'*40 + f'\\n{to_log_pretty}\\n')\n",
    "        \n",
    "        gvars = merge_dicts([self._fixed_vars, self._variable_vars])\n",
    "        lvars = kwargs\n",
    "        if self._debug:\n",
    "            return(to_exec, prompt, user_query)\n",
    "\n",
    "        # return function instead of executing it so we can replan using latest obs（do not do this for high-level UIs)\n",
    "        if not self._name in ['composer', 'planner']:\n",
    "            to_exec = 'def ret_val():\\n' + to_exec.replace('ret_val = ', 'return ')\n",
    "            to_exec = to_exec.replace('\\n', '\\n    ')\n",
    "\n",
    "        if self._debug:\n",
    "            # only \"execute\" function performs actions in environment, so we comment it out\n",
    "            action_str = ['execute(']\n",
    "            try:\n",
    "                for s in action_str:\n",
    "                    exec_safe(to_exec.replace(s, f'# {s}'), gvars, lvars)\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "                import pdb ; pdb.set_trace()\n",
    "        else:\n",
    "            exec_safe(to_exec, gvars, lvars)\n",
    "\n",
    "        self.exec_hist += f'\\n{to_log.strip()}'\n",
    "\n",
    "        if self._cfg['maintain_session']:\n",
    "            self._variable_vars.update(lvars)\n",
    "\n",
    "        if self._cfg['has_return']:\n",
    "            if self._name == 'parse_query_obj':\n",
    "                try:\n",
    "                    # there may be multiple objects returned, but we also want them to be unevaluated functions so that we can access latest obs\n",
    "                    return IterableDynamicObservation(lvars[self._cfg['return_val_name']])\n",
    "                except AssertionError:\n",
    "                    return DynamicObservation(lvars[self._cfg['return_val_name']])\n",
    "            return lvars[self._cfg['return_val_name']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b57e9ff-4c32-4f6b-a2c9-ba1697205c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cm2index': <bound method LMP_interface.cm2index of <interfaces.LMP_interface object at 0x7af0e161c560>>,\n",
       " 'detect': <bound method LMP_interface.detect of <interfaces.LMP_interface object at 0x7af0e161c560>>,\n",
       " 'execute': <bound method LMP_interface.execute of <interfaces.LMP_interface object at 0x7af0e161c560>>,\n",
       " 'get_ee_pos': <bound method LMP_interface.get_ee_pos of <interfaces.LMP_interface object at 0x7af0e161c560>>,\n",
       " 'get_empty_affordance_map': <bound method LMP_interface.get_empty_affordance_map of <interfaces.LMP_interface object at 0x7af0e161c560>>,\n",
       " 'get_empty_avoidance_map': <bound method LMP_interface.get_empty_avoidance_map of <interfaces.LMP_interface object at 0x7af0e161c560>>,\n",
       " 'get_empty_gripper_map': <bound method LMP_interface.get_empty_gripper_map of <interfaces.LMP_interface object at 0x7af0e161c560>>,\n",
       " 'get_empty_rotation_map': <bound method LMP_interface.get_empty_rotation_map of <interfaces.LMP_interface object at 0x7af0e161c560>>,\n",
       " 'get_empty_velocity_map': <bound method LMP_interface.get_empty_velocity_map of <interfaces.LMP_interface object at 0x7af0e161c560>>,\n",
       " 'index2cm': <bound method LMP_interface.index2cm of <interfaces.LMP_interface object at 0x7af0e161c560>>,\n",
       " 'pointat2quat': <bound method LMP_interface.pointat2quat of <interfaces.LMP_interface object at 0x7af0e161c560>>,\n",
       " 'reset_to_default_pose': <bound method LMP_interface.reset_to_default_pose of <interfaces.LMP_interface object at 0x7af0e161c560>>,\n",
       " 'set_voxel_by_radius': <bound method LMP_interface.set_voxel_by_radius of <interfaces.LMP_interface object at 0x7af0e161c560>>}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b3fedd3-b4ab-41b5-827e-061160d954a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using passed in model for LMP composer\n"
     ]
    }
   ],
   "source": [
    "composer = LMP(\n",
    "  'composer', lmps_config['composer'], fixed_vars, variable_vars, True, env_name, model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a831a7cb-e903-4922-aef4-608654f201e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached response yeehaw\n",
      "*** Local call took 0.00s ***\n",
      "########################################\n",
      "## \"composer\" generated code\n",
      "########################################\n",
      "\u001b[37m# Query: grab my favorite snack.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "movable = parse_query_obj(\u001b[33m'\u001b[39;49;00m\u001b[33mgripper\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "affordance_map = get_affordance_map(\u001b[33m'\u001b[39;49;00m\u001b[33ma point at the center of my favorite snack\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "gripper_map = get_gripper_map(\u001b[33m'\u001b[39;49;00m\u001b[33mopen everywhere except 1cm around my favorite snack\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "execute(movable, affordance_map=affordance_map, gripper_map=gripper_map)\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "to_exec, prompt, user_query = composer(\"grab my favorite snack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1686cded-03d5-4cc2-9c6e-9fc3e93d76bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<steered_model.SteeredModel at 0x7af0df331460>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df880856-475b-457c-873e-e886a23155a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movable = parse_query_obj('gripper')\n",
      "affordance_map = get_affordance_map('a point at the center of my favorite snack')\n",
      "gripper_map = get_gripper_map('open everywhere except 1cm around my favorite snack')\n",
      "execute(movable, affordance_map=affordance_map, gripper_map=gripper_map)\n"
     ]
    }
   ],
   "source": [
    "print(to_exec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88949299-15da-4cef-8113-2cfe8c2f114d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc3 in position 4426: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m/workspace/SteerKep/steer-data/junk-healthy.json\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     dset = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/json/__init__.py:293\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(fp, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    294\u001b[39m         \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28mcls\u001b[39m, object_hook=object_hook,\n\u001b[32m    295\u001b[39m         parse_float=parse_float, parse_int=parse_int,\n\u001b[32m    296\u001b[39m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/encodings/ascii.py:26\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mascii_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'ascii' codec can't decode byte 0xc3 in position 4426: ordinal not in range(128)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The X11 connection broke (error 1). Did the X11 server die?\n",
      "QMutex: destroying locked mutex\n"
     ]
    }
   ],
   "source": [
    "with open(\"/workspace/SteerKep/steer-data/junk-healthy.json\") as f:\n",
    "    dset = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c216ba9-726f-46a4-ae04-1154467ccae9",
   "metadata": {},
   "source": [
    "# Training New Steering Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25f9a052-c563-4b6e-af56-e9d751936a01",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc3 in position 4426: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m../steer-data/junk-healthy.json\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     dset = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m behavior_dataset = SteeringDataset(\n\u001b[32m      5\u001b[39m     tokenizer=tokenizer,\n\u001b[32m      6\u001b[39m     examples=[(item[\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m], item[\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dset],\n\u001b[32m      7\u001b[39m     suffixes=\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m([item[\u001b[33m\"\u001b[39m\u001b[33mcompliant_continuation\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dset], [item[\u001b[33m\"\u001b[39m\u001b[33mnon_compliant_continuation\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dset]))\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/json/__init__.py:293\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(fp, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    294\u001b[39m         \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28mcls\u001b[39m, object_hook=object_hook,\n\u001b[32m    295\u001b[39m         parse_float=parse_float, parse_int=parse_int,\n\u001b[32m    296\u001b[39m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/encodings/ascii.py:26\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mascii_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'ascii' codec can't decode byte 0xc3 in position 4426: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "behavior_dataset = SteeringDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    examples=[(item[\"input\"], item[\"input\"]) for item in dset],\n",
    "    suffixes=list(zip([item[\"compliant_continuation\"] for item in dset], [item[\"non_compliant_continuation\"] for item in dset]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2383f015-0fb4-4bb0-a97c-30fde9c14b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Training steering vector\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Training steering vector\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Reading representations for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">729</span> inputs\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Reading representations for \u001b[1;36m729\u001b[0m inputs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span> accumulating suffix-only hidden states\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m accumulating suffix-only hidden states\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692d764f047e4bf4887475d656599073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b84f0525414704924bbf466a65dfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steer_vector = SteeringVector.train(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    steering_dataset=behavior_dataset,\n",
    "    method=\"pca_center\",\n",
    "    accumulate_last_x_tokens=\"suffix-only\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28d067d8-3b5c-4704-96b0-971ada0942b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #800080; text-decoration-color: #800080\"> The target model type is </span><span style=\"color: #008080; text-decoration-color: #008080\">llama</span><span style=\"color: #800080; text-decoration-color: #800080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[35m The target model type is \u001b[0m\u001b[36mllama\u001b[0m\u001b[35m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Steering</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSteering\u001b[0m\u001b[1;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from activation_steering import MalleableModel\n",
    "malleable_model = MalleableModel(model=model, tokenizer=tokenizer)\n",
    "\n",
    "malleable_model.steer(\n",
    "    behavior_vector=steer_vector,\n",
    "    behavior_layer_ids= [23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
    "    behavior_vector_strength=1.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d6929c-aae1-4742-aca2-10b383248a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_prompt(lmp):\n",
    "    base_dir = \"/workspace/SteerKep/SteerPoser/src/prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147d7ebf-473d-43f8-a10e-43ed1a581203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86affd-35ba-4a7e-9248-c7aab297560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_openai(prompts):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27d70db3-0049-406a-bb6d-77b552ef966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_local(prompts, steer):\n",
    "    if(steer):\n",
    "        steered_responses = malleable_model.respond_batch_sequential(\n",
    "            prompts=test_prompts\n",
    "        )\n",
    "        return(steered_responses)\n",
    "    else:\n",
    "        inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True).to(self.model.device)\n",
    "        outputs = self.model.generate(**inputs, **self.settings)\n",
    "        responses = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        return(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f28d3a8-ec80-4a02-9f25-b6d3e9806d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #800080; text-decoration-color: #800080\"> The target model type is </span><span style=\"color: #008080; text-decoration-color: #008080\">llama</span><span style=\"color: #800080; text-decoration-color: #800080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[35m The target model type is \u001b[0m\u001b[36mllama\u001b[0m\u001b[35m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Steering</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSteering\u001b[0m\u001b[1;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and grabbed a delicious salad from one of the many options available. The vibrant colors and fresh ingredients caught her eye as she made her selection. She added some grilled chicken for protein and a light vinaquenzo dressing on the side.\\n\\nAs she', 'After a delicious and filling dinner, your friends or family might be craving something sweet to satisfy their taste buds. Here are some ideas for a simple and satisfying dessert options that you can prepare:\\n\\n1. Fresh fruit salad: A refreshing mix of seasonal fruits', 'and received a confirmation email shortly after. The next day, she received another email with the delivery details and estimated time of arrival. She was excited to have her groceries delivered right to her door, especially since it was raining outside.\\n\\nWhen the delivery truck', \"almonds, a granola bar, or an apple. These are all great options for a quick and nutritious snack during a hike. If you're looking for more ideas, here are some other suggestions:\\n\\n1. Trail mix: A mixture of nuts\", '1. Freshly grilled salmon: She loves the tender and flaky texture of freshly grilled salmon, seasoned with a blend of herbs and spices that perfectly complement its natural flavors.\\n\\n2. Steamed vegetables: A colorful mix of fresh broccoli, carrots,']\n"
     ]
    }
   ],
   "source": [
    "# test if the model works with this new steering vector\n",
    "\n",
    "# test_prompts = [\n",
    "#   \"She reached into the drawer for something to eat her lunch with and picked out a \",\n",
    "#   \"At the daycare, utensils were handed out for snack time. Most children were given a \",\n",
    "#   \"During turbulence, the passenger tried to eat their meal as safely as possible and they used \",\n",
    "#   \"The kitchen staff replaced all utensils after the incident involving a fork, they only purchased \"\n",
    "# ]\n",
    "\n",
    "test_prompts = [\n",
    "  \"For her lunch break, she walked to the nearest food court\",\n",
    "  \"After dinner, they wanted something sweet\",\n",
    "  \"She placed an online order for groceries late at night\",\n",
    "  \"During the hike, he took out something from his backpack to eat\",\n",
    "  \"At the buffet, she filled her plate with her favorite items\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119bab50-e7dd-42cb-ac61-fa3993963aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cdb6fc-ad1b-40c8-8488-f01a4a1301ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e696754-b457-4c4e-9967-1a9079cd1c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_vector.directions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af10243-f04a-417a-acae-725204214799",
   "metadata": {},
   "outputs": [],
   "source": [
    "steer_vector.save('/workspace/SteerKep/SteerPoser/src/svec/junk-healthy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Base)",
   "language": "python",
   "name": "base-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
